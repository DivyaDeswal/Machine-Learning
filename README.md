# ğŸ¤– Machine Learning Algorithms
Welcome to my Machine Learning Algorithms repository! ğŸš€ This repo contains Jupyter notebooks implementing Supervised and Unsupervised Machine Learning algorithms with explanations and code examples.

## ğŸ“‚ Algorithms Included
### 1ï¸âƒ£ Linear Regression
ğŸ“Œ Type: Supervised Learning (Regression)

ğŸ“Œ Use Case: Predicting continuous values (e.g., house prices, sales forecasting).

ğŸ”§ Libraries Used: sklearn, pandas, numpy, matplotlib.

### 2ï¸âƒ£ Logistic Regression
ğŸ“Œ Type: Supervised Learning (Classification)

ğŸ“Œ Use Case: Binary classification problems (e.g., spam detection, disease prediction).

ğŸ”§ Libraries Used: sklearn, numpy, matplotlib.

### 3ï¸âƒ£ Decision Tree Algorithm
ğŸ“Œ Type: Supervised Learning (Classification & Regression)

ğŸ“Œ Use Case: Customer segmentation, credit risk assessment.

ğŸ“Œ How it Works:

The model splits data into branches based on feature conditions.
Uses Gini Index or Entropy to determine splits.

Forms a tree structure where leaves represent outcomes.

ğŸ”§ Libraries Used: sklearn.tree, pandas, matplotlib.

### 4ï¸âƒ£ Random Forest Algorithm (Iris Dataset)
ğŸ“Œ Type: Supervised Learning (Classification)

ğŸ“Œ Use Case: Complex classification problems (e.g., medical diagnosis, image recognition).

ğŸ“Œ How it Works:

An ensemble method combining multiple decision trees.
Each tree votes, and the most common prediction is selected (majority voting).
Reduces overfitting compared to a single decision tree.

ğŸ”§ Libraries Used: sklearn.ensemble.RandomForestClassifier, pandas, matplotlib.

### 5ï¸âƒ£ K-Means & Hierarchical Clustering
ğŸ“Œ Type: Unsupervised Learning (Clustering)

ğŸ“Œ Use Case: Customer segmentation, anomaly detection.

ğŸ“Œ How it Works:

K-Means Clustering
Defines K clusters (centroids).
Assigns each data point to the nearest cluster center.
Updates centroids until convergence.

#### Hierarchical Clustering
Creates a hierarchy of clusters using Agglomerative (bottom-up) or Divisive (top-down) methods.
Uses Dendrograms to visualize cluster merging.

ğŸ”§ Libraries Used: sklearn.cluster, scipy.cluster.hierarchy, matplotlib, seaborn.

#### ğŸ› ï¸ Tools & Libraries Used
Python ğŸ

Jupyter Notebook

Scikit-Learn (sklearn)

Pandas, NumPy, Matplotlib, Seaborn

#### ğŸ“Œ How to Use
Clone the repository:
git clone https://github.com/DivyaDeswal/Machine-Learning.git

Open the notebooks in Jupyter or Google Colab.
Run the code and experiment with different models

## ğŸ“‚ Classifiers
### Boosting Ensemble Techniques

Boosting is a powerful ensemble technique that combines multiple weak classifiers to form a strong classifier. The following methods are implemented in this repository:

### 1ï¸âƒ£ Adaboost (Adaptive Boosting): 
A technique that adjusts weights iteratively to emphasize misclassified instances.

### 2ï¸âƒ£ XGBoost (Extreme Gradient Boosting): 
An optimized gradient boosting library designed for speed and performance.

### 3ï¸âƒ£ LightGBM (Light Gradient Boosting Machine): 
A gradient boosting framework that uses tree-based learning algorithms for faster training on large datasets.

### Dimensionality Reduction Techniques

Dimensionality reduction is a technique used to reduce the number of input variables in a dataset while retaining as much meaningful information as possible. This is particularly useful in handling high-dimensional data.

### Principal Component Analysis (PCA): 
PCA is a widely used dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space by retaining the most significant features.

The provided implementation applies PCA to the liver patient dataset, demonstrating how it helps in feature selection and improving model efficiency.
